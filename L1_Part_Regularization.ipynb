{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUWnpdG_Uw6G"
      },
      "source": [
        "# Optional Lab - Regularized Cost and Gradient\n",
        "\n",
        "## Course Introduction\n",
        "\n",
        "**Course:** Big Data And Analytics\n",
        "**Years:** 2023-2024\n",
        "**University:** Polytechnic University of Bari, Italy\n",
        "**Instructor:** Dr. Yashar Deldjoo\n",
        "**Topic:** Regularization\n",
        "\n",
        "Welcome to the 'Big Data And Analytics' course, offered by the Polytechnic University of Bari for the academic years 2023-2024. This course is designed to equip students with the fundamental concepts and tools necessary to understand and work with large and complex data sets.\n",
        "\n",
        "We will guide you through the intricacies of data analytics, from theoretical foundations to practical applications. This notebook focuses on the implementation of Gradient Descent, a cornerstone algorithm for optimizing complex models.\n",
        "\n",
        "Feel free to reach out with questions and engage actively in your learning journey. Let's dive into the world of big data together!\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yasdel/Big_Data_And_Analytics/blob/main/L1_Part_Final)"
      ],
      "id": "OUWnpdG_Uw6G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbUWsT-2Uw6K"
      },
      "source": [
        "## Goals\n",
        "In this lab, you will:\n",
        "- extend the previous linear and logistic cost functions with a regularization term.\n",
        "- rerun the previous example of over-fitting with a regularization term added.\n"
      ],
      "id": "GbUWsT-2Uw6K"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8q0FslssUw6L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "id": "8q0FslssUw6L"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KH2sxh70Uw6M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n"
      ],
      "id": "KH2sxh70Uw6M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsRE0hNyUw6N"
      },
      "source": [
        "# Adding regularization\n",
        "<img align=\"Left\" src=\"./images/C1_W3_LinearGradientRegularized.png\"  style=\" width:400px; padding: 10px; \" >\n",
        "<img align=\"Center\" src=\"./images/C1_W3_LogisticGradientRegularized.png\"  style=\" width:400px; padding: 10px; \" >\n",
        "\n",
        "The slides above show the cost and gradient functions for both linear and logistic regression. Note:\n",
        "- Cost\n",
        "    - The cost functions differ significantly between linear and logistic regression, but adding regularization to the equations is the same.\n",
        "- Gradient\n",
        "    - The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of $f_{wb}$."
      ],
      "id": "hsRE0hNyUw6N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUmke4ZUw6N"
      },
      "source": [
        "## Cost functions with regularization\n",
        "### Cost function for regularized linear regression\n",
        "\n",
        "The equation for the cost function regularized linear regression is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$\n",
        "\n",
        "\n",
        "Compare this to the cost function without regularization (which you implemented in  a previous lab), which is of the form:\n",
        "\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$\n",
        "\n",
        "The difference is the regularization term,  <span style=\"color:blue\">\n",
        "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span>\n",
        "    \n",
        "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.\n",
        "\n",
        "Below is an implementation of equations (1) and (2). Note that this uses a *standard pattern for this course*,   a `for loop` over all `m` examples."
      ],
      "id": "LOUmke4ZUw6N"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jaJZNlSFUw6O"
      },
      "outputs": [],
      "source": [
        "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost\n",
        "    \"\"\"\n",
        "\n",
        "    m  = X.shape[0]\n",
        "    n  = len(w)\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
        "        cost = cost + (f_wb_i - y[i])**2                               #scalar\n",
        "    cost = cost / (2 * m)                                              #scalar\n",
        "\n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)                                          #scalar\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
        "\n",
        "    total_cost = cost + reg_cost                                       #scalar\n",
        "    return total_cost                                                  #scalar"
      ],
      "id": "jaJZNlSFUw6O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rwcJwOoUw6O"
      },
      "source": [
        "Run the cell below to see it in action."
      ],
      "id": "7rwcJwOoUw6O"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoERd3wOUw6P",
        "outputId": "0c3b5b74-84b7-4f3a-85aa-0bd626a386b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularized cost: 0.07917239320214277\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,6)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(\"Regularized cost:\", cost_tmp)"
      ],
      "id": "BoERd3wOUw6P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deJE8NyhUw6Q"
      },
      "source": [
        "**Expected Output**:\n",
        "<table>\n",
        "  <tr>\n",
        "    <td> <b>Regularized cost: </b> 0.07917239320214275 </td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "id": "deJE8NyhUw6Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9AcnM6IUw6Q"
      },
      "source": [
        "### Cost function for regularized logistic regression\n",
        "For regularized **logistic** regression, the cost function is of the form\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$\n",
        "\n",
        "Compare this to the cost function without regularization (which you implemented in  a previous lab):\n",
        "\n",
        "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$\n",
        "\n",
        "As was the case in linear regression above, the difference is the regularization term, which is    <span style=\"color:blue\">\n",
        "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span>\n",
        "\n",
        "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice."
      ],
      "id": "s9AcnM6IUw6Q"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7YchOBU9Uw6Q"
      },
      "outputs": [],
      "source": [
        "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost\n",
        "    \"\"\"\n",
        "\n",
        "    m,n  = X.shape\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
        "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
        "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
        "\n",
        "    cost = cost/m                                                      #scalar\n",
        "\n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)                                          #scalar\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
        "\n",
        "    total_cost = cost + reg_cost                                       #scalar\n",
        "    return total_cost                                                  #scalar"
      ],
      "id": "7YchOBU9Uw6Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewV4LLjkUw6R"
      },
      "source": [
        "Run the cell below to see it in action."
      ],
      "id": "ewV4LLjkUw6R"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3SiA34YUw6R",
        "outputId": "7a9d5b40-7a27-481e-9fa1-0cc8d1d86406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularized cost: 0.6850849138741673\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,6)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(\"Regularized cost:\", cost_tmp)"
      ],
      "id": "k3SiA34YUw6R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1z7TMLvUw6R"
      },
      "source": [
        "**Expected Output**:\n",
        "<table>\n",
        "  <tr>\n",
        "    <td> <b>Regularized cost: </b> 0.6850849138741673 </td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "id": "d1z7TMLvUw6R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqdOd2BIUw6S"
      },
      "source": [
        "## Gradient descent with regularization\n",
        "The basic algorithm for running gradient descent does not change with regularization, it is:\n",
        "$$\\begin{align*}\n",
        "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
        "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\\n",
        "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
        "&\\rbrace\n",
        "\\end{align*}$$\n",
        "Where each iteration performs simultaneous updates on $w_j$ for all $j$.\n",
        "\n",
        "What changes with regularization is computing the gradients."
      ],
      "id": "pqdOd2BIUw6S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwGD3fQNUw6S"
      },
      "source": [
        "### Computing the Gradient with regularization (both linear/logistic)\n",
        "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$.\n",
        "$$\\begin{align*}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
        "\\end{align*}$$\n",
        "\n",
        "* m is the number of training examples in the data set      \n",
        "* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n",
        "\n",
        "      \n",
        "* For a  <span style=\"color:blue\"> **linear** </span> regression model  \n",
        "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
        "* For a <span style=\"color:blue\"> **logistic** </span> regression model  \n",
        "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
        "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
        "    where $g(z)$ is the sigmoid function:  \n",
        "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
        "    \n",
        "The term which adds regularization is  the <span style=\"color:blue\">$\\frac{\\lambda}{m} w_j $</span>."
      ],
      "id": "KwGD3fQNUw6S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYg3G4drUw6S"
      },
      "source": [
        "### Gradient function for regularized linear regression"
      ],
      "id": "lYg3G4drUw6S"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rjvM1HJJUw6S"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_linear_reg(X, y, w, b, lambda_):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "id": "rjvM1HJJUw6S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJKHvnZ4Uw6S"
      },
      "source": [
        "Run the cell below to see it in action."
      ],
      "id": "PJKHvnZ4Uw6S"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHGFqFzZUw6T",
        "outputId": "3e928028-5670-48aa-da45-f1214a2f71fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dj_db: 0.6648774569425726\n",
            "Regularized dj_dw:\n",
            " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(f\"dj_db: {dj_db_tmp}\", )\n",
        "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
      ],
      "id": "rHGFqFzZUw6T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0rg17OyUw6T"
      },
      "source": [
        "**Expected Output**\n",
        "```\n",
        "dj_db: 0.6648774569425726\n",
        "Regularized dj_dw:\n",
        " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n",
        " ```"
      ],
      "id": "s0rg17OyUw6T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytO9EKmaUw6T"
      },
      "source": [
        "### Gradient function for regularized logistic regression"
      ],
      "id": "ytO9EKmaUw6T"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wQQXQSM4Uw6T"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_logistic_reg(X, y, w, b, lambda_):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns\n",
        "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))                            #(n,)\n",
        "    dj_db = 0.0                                       #scalar\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
        "        err_i  = f_wb_i  - y[i]                       #scalar\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
        "        dj_db = dj_db + err_i\n",
        "    dj_dw = dj_dw/m                                   #(n,)\n",
        "    dj_db = dj_db/m                                   #scalar\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw\n"
      ],
      "id": "wQQXQSM4Uw6T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktN6rbpIUw6T"
      },
      "source": [
        "Run the cell below to see it in action."
      ],
      "id": "ktN6rbpIUw6T"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eA7aFs0Uw6U",
        "outputId": "52b4add8-68e6-4c18-f11e-2ea6797bd960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dj_db: 0.341798994972791\n",
            "Regularized dj_dw:\n",
            " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(f\"dj_db: {dj_db_tmp}\", )\n",
        "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
      ],
      "id": "5eA7aFs0Uw6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaA3JZKwUw6U"
      },
      "source": [
        "**Expected Output**\n",
        "```\n",
        "dj_db: 0.341798994972791\n",
        "Regularized dj_dw:\n",
        " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n",
        " ```"
      ],
      "id": "IaA3JZKwUw6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZv9iWjCUw6U"
      },
      "source": [
        "## Congratulations!\n",
        "You have:\n",
        "- examples of cost and gradient routines with regularization added for both linear and logistic regression\n",
        "- developed some intuition on how regularization can reduce over-fitting"
      ],
      "id": "MZv9iWjCUw6U"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MhQrwZSRUw6V"
      },
      "outputs": [],
      "source": [],
      "id": "MhQrwZSRUw6V"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}